{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c217dbc-a666-402e-9deb-3f07172ae3fe",
   "metadata": {},
   "source": [
    "<h1>CNN AutoEncoder</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d818f-4b2a-4a68-873a-7e65a5419bcd",
   "metadata": {},
   "source": [
    "In this project we will build a cnn autoencoder from scratch with pytorch to reconstruct images.\n",
    "the dataset will be the MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b3537f-4dc7-4fb6-99af-fe8834ee60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torchvision import datasets , transforms \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d8be3d-8d17-4291-9c08-32fc0984ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "mnist_data = datasets.MNIST(root= \"./data\" , train = True, download = False, transform =transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset = mnist_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2668e0ac-fb25-48c5-b076-9567540f9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the sizes for training, validation, and testing\n",
    "# train_size = int(0.8 * len(mnist_data))  # 80% for training\n",
    "# val_size = int(0.1 * len(mnist_data))    # 10% for validation\n",
    "# test_size = len(mnist_data) - train_size - val_size  # 10% for testing\n",
    "# # Use random_split to create subsets\n",
    "# train_data, val_data, test_data = torch.utils.data.random_split(\n",
    "#     mnist_data, [train_size, val_size, test_size]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138f6c4-b737-4c82-ab54-a27a62013972",
   "metadata": {},
   "source": [
    "<h1>Model's Architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d0a6ea-dc6f-4df6-8be2-297a0ee0f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28 ,128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128 , 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12, 3)\n",
    "        ); \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3 ,12), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12 , 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, 28*28), \n",
    "            nn.Sigmoid()\n",
    "        );  \n",
    "    def forward (self , x):\n",
    "        embedding = self.encoder(x)\n",
    "        output = self.decoder (embedding);\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5aed27-4e26-4062-a0f9-d9343909a726",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    Training the Model\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe14251-2409-4def-80e1-ea90c386344b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let us set the device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5378b72-16df-4e8c-a706-3d64f08a615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiation \n",
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c18f49-7c57-4f48-8843-0185aaefb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "#optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#number of epochs \n",
    "epochs = 20 \n",
    " \n",
    "#training model\n",
    "def train (model , data):\n",
    "    model = model.to(device) \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"started epoch {epoch+1}\");\n",
    "        running_loss= 0.0 # to track the loss \n",
    "        for images ,_  in data_loader: \n",
    "            images = images.view(images.size(0), -1).to(device)            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model (images)\n",
    "            loss = criterion (outputs, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(data_loader):.4f}\") \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eba42aad-3ca9-4413-975e-98ffdc164920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started epoch 1\n",
      "Epoch [1/20], Loss: 0.0366\n",
      "started epoch 2\n",
      "Epoch [2/20], Loss: 0.0357\n",
      "started epoch 3\n",
      "Epoch [3/20], Loss: 0.0350\n",
      "started epoch 4\n",
      "Epoch [4/20], Loss: 0.0345\n",
      "started epoch 5\n",
      "Epoch [5/20], Loss: 0.0340\n",
      "started epoch 6\n",
      "Epoch [6/20], Loss: 0.0337\n",
      "started epoch 7\n",
      "Epoch [7/20], Loss: 0.0333\n",
      "started epoch 8\n",
      "Epoch [8/20], Loss: 0.0331\n",
      "started epoch 9\n",
      "Epoch [9/20], Loss: 0.0328\n",
      "started epoch 10\n",
      "Epoch [10/20], Loss: 0.0326\n",
      "started epoch 11\n",
      "Epoch [11/20], Loss: 0.0324\n",
      "started epoch 12\n",
      "Epoch [12/20], Loss: 0.0322\n",
      "started epoch 13\n",
      "Epoch [13/20], Loss: 0.0320\n",
      "started epoch 14\n",
      "Epoch [14/20], Loss: 0.0318\n",
      "started epoch 15\n",
      "Epoch [15/20], Loss: 0.0317\n",
      "started epoch 16\n",
      "Epoch [16/20], Loss: 0.0315\n",
      "started epoch 17\n",
      "Epoch [17/20], Loss: 0.0314\n",
      "started epoch 18\n",
      "Epoch [18/20], Loss: 0.0313\n",
      "started epoch 19\n",
      "Epoch [19/20], Loss: 0.0312\n",
      "started epoch 20\n",
      "Epoch [20/20], Loss: 0.0311\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(model , data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2a561-480b-45cd-a1f2-6900e04d10b6",
   "metadata": {},
   "source": [
    "<h1>Testing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d20e92eb-e40d-4171-9026-0d87c3b96143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#data iterator \n",
    "dataIter = iter(data_loader)\n",
    "images,  labels = next(dataIter)\n",
    "print(torch.min(images) , torch.max(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9f13a-fc72-40a4-9c28-1953cac779c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the pixel values are between 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfa4a3c4-b227-426a-8656-d73c3eb425fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tDRND1LxFqkOm6VatcXUpwqAgD6kngD6111z8JNcgsNWuI9R0a5l0mPzL21guiZYhySCCoGQFJ689sniuBor3vwHYXHir4cWmn+DdSm0C+s5wusTCLDXQbdhlkUbmKjOF464PYnjPHvjq3ubObw7oa3It2ZBqWoXYxcag8Y2rv7hRjgH0HTv5vWjoGjy+IPEOn6RA2yS8nSEORnbk4LY9hz+FfSjQDwKbOCKSbSvCHh9GlnnkISXVbkg4UAcsPwAJHGQAR80a1qC6rruoaikXkrdXMk4j/uBmJxx6ZqjT4ZpbadJoJXiljYMjoxVlI6EEdDVm+1bUtT2fb9Qu7vZ93z5mk2/TJ4qnRX/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABF0lEQVR4AWNgGORAa+qxfzv+3TPH5szln79vmb/u79/vihiy8hnX/n2pZOC99feRGJokm8/3v39f3fzMYPD3rwaanPSOv59KIvisG/Kf/8xlRJM0//tXGyjEPPXviyVoUgwMG/7+zWZgEJ/w9+86DDkGjU9/f/tpXfn7rJUbU5JBbfXfn5//7QWZjQWIffv796wCkgQTgs2axcHIaBSFEEBisUf+/fv+/b9VSEIIJtDrjwz7/z0VRQghWGv+/vVmmPvvOkIEzmI78/tvFaPOD6ySi//+fabFuvzvAyO4ejhD+s3ft/IMs/7+jYYLIRjWf/9Oar35528vQgjBqvoLAt+SuRBCQBYLhLfGlv2T8trNZ1HkaMUBAK3KdZ1xp6eHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image \n",
    "transform_toPil= transforms.ToPILImage()\n",
    "img = transform_toPil(images[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1584f8ab-abda-4a8c-9204-c8e68c9f3b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#squeeze (remove the batch dimension ) so that we can flatten it then pass it to the model \n",
    "pic = images[0].squeeze()\n",
    "# Flatten the input tensor to shape [1, 28*28]\n",
    "input_tensor_flattened = pic.view(1, -1)  # Shape: [1, 784]\n",
    "input_tensor_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39a3aa0d-5459-4849-948c-0d9d3299e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_flattened = input_tensor_flattened.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9533a63d-c3b9-4b67-879a-aaac43a0540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = trained_model(input_tensor_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "392c40b9-37af-4103-abb2-5c8e174c905f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "pred = prediction.view(28,28)\n",
    "print(pred.shape)\n",
    "pred = pred.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c31b43f2-ec83-49e8-afb1-80f0b49dde60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f71221e-84a5-4373-9777-433510192a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tTRfDuqeIbr7Pplo879yOAv1PatvVfhxruj2LXVx9nZF++I5NxX61yJBBwetJXtXgKxvdU+Htxa6O32e4aX99N0O361zPivxTDYaf/wj+mSvNs4uLljzI3fHtXnjHcSTUltEJrmONm2hmAzX0VoFnqWix2kECR23hyC3MlxcFh+8JHevANcmguNdvprbPkPO5jz1254rPoBIOQcGrz61qktr9lk1G6a3/wCeTSsV/KqNFf/Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABmElEQVR4Ab2RSU/CQBiGZzqdWjWllUUkIgouJArGGE28qfHi0T/qzaNxRb140WhQlghojBtSBNLSTmcsSyl6NXFOkzx5vu99ZwD4/wN/ruQQ4gVCDYsyG/yAnKhMhuOiXioW33Ub866JBO/iRtJjkMr03SB4a1LWgxCJ4c3tkFUpM+ANSkMDJnVNDgd31n2v6XsaiIq8aFEA+8zh+Tkxf36phmTceC5+EcYA190JeY8/YOZuSvpoQv64ytapHdXZCRFAuvHZkGeWx3KHt7V2GQcyYla/FCEiJaPpg+uyyag90oX1sskvRQIov/tQbYLWG/QgYGa9MjVFtKP9bM3oBnECAYgw8vpD6n1BI13WMyGUF5MK0SzYJO2RfWM5KC6sjBTex7AHtd+8LXcCQYQiK/LdGdryR5H7Fw6UV2NPp5lxTRrnWyU6pwOREJwg2bQm+XG96SCnChzwSdZwDK0Fqin1t2mRGgsFE8okPr7oFXFMZuiP8QlpqHqyp7pNuj2p8XLFzSqNTKqo9plObogFjKFmmJab5w+3b7T9pj1/+9vrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic= transform_toPil(pred)\n",
    "pic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
